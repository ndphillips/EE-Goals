yarrr::pirateplot(risky.ag ~ strategy + environment, data = sim.dm)
yarrr::pirateplot(risky.ug ~ strategy + environment, data = sim.dm)
# Set working directory
setwd(rprojroot::is_rstudio_project$find_file())
# Load libraries
library(dplyr)
library(yarrr)
library(snowfall)
library(snow)
# Read learning functions
source("r/learning_functions.R")
# sim.dm is a design matrix of simulations. All combinations of the
#  parameters will be simulated
sim.dm <- expand.grid(goal = c(70),                               # Goal
n.trials = c(25),                           # Trials in game
environment = 1:3,                          # Option environment
strategy = c("ev", "rsf"),                  # General strategy
selection.strat = c("egreedy"),  # Selection strategy
sim = 1:300)                                # Simulations
# Each statistical environment is defined as a dataframe of means and standard deviations
environments <- list(data.frame(mean = c(4, 4),
sd = c(1, 5)),
data.frame(mean = c(4, 2),
sd = c(1, 5)),
data.frame(mean = c(2, 4),
sd = c(1, 5)))
# sim.dm.fun() runs the simulation for a given parameter combination and returns
#   aggregate statistics
sim.dm.fun <- function(x) {
goal.i <- sim.dm$goal[x]
n.trials.i <- sim.dm$n.trials[x]
environment.i <- sim.dm$environment[x]
option.mean.i <- environments[[environment.i]]$mean
option.sd.i <- environments[[environment.i]]$sd
strategy.i <- sim.dm$strategy[x]
selection.strat.i <- sim.dm$selection.strat[x]
sim.i <- rl.sim.fun(n.trials = n.trials.i,     # Trials in game
option.mean = option.mean.i,   # Mean of each option
option.sd = option.sd.i,   # SD of each option
prior.exp.start = rep(0, length(option.mean)),
prior.sd.start = 1,
goal = goal.i,
epsilon = .2,                        # p(explore | selection.strat = "egreedy")
theta = 1,
alpha = .2,
plot = FALSE,
strategy = strategy.i,
ylim = c(0, 100),
selection.strat = selection.strat.i)
# Extract key statistics
option.risky <- which(option.sd.i == max(option.sd.i))
final.points.i <- sim.i$outcome.cum[n.trials.i]
reach.goal.i <- final.points.i > goal.i
risky.i <- mean(sim.i$selection == option.risky)
# choosing risky option when above goal (ag) or under goal (ug)
risky.ug.i <- mean(sim.i$selection[sim.i$outcome.cum < goal.i] == option.risky)
risky.ag.i <- mean(sim.i$selection[sim.i$outcome.cum > goal.i] == option.risky)
output <- data.frame("final.points" = final.points.i,
"reach.goal" = reach.goal.i,
"risky" = risky.i,
"risky.ug" = risky.ug.i,
"risky.ag" = risky.ag.i)
return(output)
}
# Run all simulations from sim.dm, then coerce into a dataframe
snowfall::sfInit(parallel = TRUE, cpus = 4)
snowfall::sfExportAll()
sim.result.ls <- snowfall::sfClusterApplySR(1:nrow(sim.dm), fun = sim.dm.fun, perUpdate = 1)
snowfall::sfStop()   # stop cluster
# Combine simulation results with sim.dm
sim.result.df <- do.call(rbind, sim.result.ls)
sim.dm <- cbind(sim.dm, sim.result.df)
# Calculate aggreagte statistics across simulations
sim.dm.agg <- sim.dm %>% group_by(goal, n.trials, environment, strategy) %>%
summarise(
reachgoal.p = mean(reach.goal, na.rm = TRUE),
risky.mean= mean(risky, na.rm = TRUE),
risky.ag.mean = mean(risky.ag, na.rm = TRUE),
risky.ug.mean = mean(risky.ug, na.rm = TRUE),
points.mean = mean(final.points, na.rm = TRUE)
)
# Set working directory
setwd(rprojroot::is_rstudio_project$find_file())
# Load libraries
library(dplyr)
library(yarrr)
library(snowfall)
library(snow)
# Read learning functions
source("r/learning_functions.R")
# sim.dm is a design matrix of simulations. All combinations of the
#  parameters will be simulated
sim.dm <- expand.grid(goal = c(80),                               # Goal
n.trials = c(25),                           # Trials in game
environment = 1:3,                          # Option environment
strategy = c("ev", "rsf"),                  # General strategy
selection.strat = c("egreedy"),  # Selection strategy
sim = 1:300)                                # Simulations
# Each statistical environment is defined as a dataframe of means and standard deviations
environments <- list(data.frame(mean = c(3, 3),
sd = c(1, 5)),
data.frame(mean = c(3, 2),
sd = c(1, 5)),
data.frame(mean = c(2, 3),
sd = c(1, 5)))
# sim.dm.fun() runs the simulation for a given parameter combination and returns
#   aggregate statistics
sim.dm.fun <- function(x) {
goal.i <- sim.dm$goal[x]
n.trials.i <- sim.dm$n.trials[x]
environment.i <- sim.dm$environment[x]
option.mean.i <- environments[[environment.i]]$mean
option.sd.i <- environments[[environment.i]]$sd
strategy.i <- sim.dm$strategy[x]
selection.strat.i <- sim.dm$selection.strat[x]
sim.i <- rl.sim.fun(n.trials = n.trials.i,     # Trials in game
option.mean = option.mean.i,   # Mean of each option
option.sd = option.sd.i,   # SD of each option
prior.exp.start = rep(0, length(option.mean)),
prior.sd.start = 1,
goal = goal.i,
epsilon = .2,                        # p(explore | selection.strat = "egreedy")
theta = 1,
alpha = .2,
plot = FALSE,
strategy = strategy.i,
ylim = c(0, 100),
selection.strat = selection.strat.i)
# Extract key statistics
option.risky <- which(option.sd.i == max(option.sd.i))
final.points.i <- sim.i$outcome.cum[n.trials.i]
reach.goal.i <- final.points.i > goal.i
risky.i <- mean(sim.i$selection == option.risky)
# choosing risky option when above goal (ag) or under goal (ug)
risky.ug.i <- mean(sim.i$selection[sim.i$outcome.cum < goal.i] == option.risky)
risky.ag.i <- mean(sim.i$selection[sim.i$outcome.cum > goal.i] == option.risky)
output <- data.frame("final.points" = final.points.i,
"reach.goal" = reach.goal.i,
"risky" = risky.i,
"risky.ug" = risky.ug.i,
"risky.ag" = risky.ag.i)
return(output)
}
# Run all simulations from sim.dm, then coerce into a dataframe
snowfall::sfInit(parallel = TRUE, cpus = 4)
snowfall::sfExportAll()
sim.result.ls <- snowfall::sfClusterApplySR(1:nrow(sim.dm), fun = sim.dm.fun, perUpdate = 1)
snowfall::sfStop()   # stop cluster
# Combine simulation results with sim.dm
sim.result.df <- do.call(rbind, sim.result.ls)
sim.dm <- cbind(sim.dm, sim.result.df)
# Calculate aggreagte statistics across simulations
sim.dm.agg <- sim.dm %>% group_by(goal, n.trials, environment, strategy) %>%
summarise(
reachgoal.p = mean(reach.goal, na.rm = TRUE),
risky.mean= mean(risky, na.rm = TRUE),
risky.ag.mean = mean(risky.ag, na.rm = TRUE),
risky.ug.mean = mean(risky.ug, na.rm = TRUE),
points.mean = mean(final.points, na.rm = TRUE)
)
yarrr::pirateplot(reach.goal ~ strategy + environment, data = sim.dm)
yarrr::pirateplot(risky ~ strategy + environment, data = sim.dm)
yarrr::pirateplot(risky.ag ~ strategy + environment, data = sim.dm)
yarrr::pirateplot(risky.ug ~ strategy + environment, data = sim.dm)
# TESTING
n.trials = 25                # Trials in game
option.mean = c(2, 3)   # Mean of each option
option.sd = c(1, 5)       # SD of each option
prior.exp.start = rep(0, 2)  # Prior expectations
prior.sd.start = 1            # Prior standard deviation
goal = 50                    # Goal
epsilon = .1                 # epsilon parameter for egreedy.fun
theta = .5                    # theta parameter for softmax.fun
alpha = .2                    # alpha updating rate for rw.fun
selection.strat = "softmax"   # softmax or egreedy
strategy = "rsf"               # Either ev or rsf
plot = FALSE
ylim = NULL
# Get some game parameters from inputs
n.options <- length(option.mean)
# Create outcome matrix giving the outcome on each trial for each option
outcome.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
for(option.i in 1:n.options) {
outcome.mtx[,option.i] <- rnorm(n = n.trials,
mean = option.mean[option.i],
sd = option.sd[option.i])
}
# Create exp.prior and exp.new matrices
#  These will hold the agent's expectation (either prior or new)
#   of each option on each trial
exp.prior.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
exp.prior.mtx[1,] <- prior.exp.start
exp.new.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# Now create some matrices to store values
selection.v <- rep(NA, n.trials)      # Actual selections
outcome.v <- rep(NA, n.trials)        # Actual outcomes
selprob.mtx <- as.data.frame(matrix(NA,             # Selection probabilities
nrow = n.trials,
ncol = n.options))
names(selprob.mtx) <- paste0("sel.", letters[1:n.options])
gt.mtx <- as.data.frame(matrix(NA,             # p get there
nrow = n.trials,
ncol = n.options))
names(gt.mtx) <- paste0("gt.", letters[1:n.options])
reward.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# RUN SIMULATION!
for(trial.i in 1:n.trials) {
# STEP 0: Get prior expectations for current trial
exp.prior.i <- exp.prior.mtx[trial.i,]
# Determine probability of reaching goal for each option
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
sd.observed <- sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed[is.na(sd.observed)] <- prior.sd.start
trials.left <- n.trials - trial.i
p.getthere <- p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
# STEP 1: SELECT AN OPTION
# Selection probabilities
# EV strategy: Select according to expectations
if(strategy == "ev") {
if(selection.strat == "softmax") {
selprob.i <- softmax.fun(exp.current = exp.prior.i,
theta = theta)
}
if(selection.strat == "egreedy") {
selprob.i <- egreedy.fun(exp.current = exp.prior.i,
epsilon = epsilon)
}
}
# RSF strategy: Select according to probability of reaching the goal
if(strategy == "rsf") {
if(selection.strat == "softmax") {
if(mean(is.finite(p.getthere)) == 1) {
if(sum(p.getthere) == 0) {
selprob.i <- rep(1 / n.options, n.options)} else {
selprob.i <- p.getthere / sum(p.getthere)
}
} else {selprob.i <- rep(1 / n.options, n.options)}
}
if(selection.strat == "egreedy") {
selprob.i <- egreedy.fun(p.getthere, epsilon)
}
}
# Select an option
selection.i <- sample(1:n.options, size = 1, prob = selprob.i)
# Get outcome from selected option
outcome.i <- outcome.mtx[trial.i, selection.i]
# STEP 3: CREATE NEW EXPECTANCIES
# Create a new.inf vector with NAs except for outcome of selected option
new.inf <- rep(NA, n.options)
new.inf[selection.i] <- outcome.i
# Get new expectancies
new.exp.i <- rw.fun(exp.prior = exp.prior.i,
new.inf = new.inf,
alpha = alpha)
# assign new expectatations to exp.new.mtx[trial.i,]
#  and prior.expecation.mtx[trial.i + 1,]
exp.new.mtx[trial.i,] <- new.exp.i
if(trial.i < n.trials) {
exp.prior.mtx[trial.i + 1,] <- new.exp.i
}
# Save some values
selprob.mtx[trial.i,] <- selprob.i  # Selection probabilities
selection.v[trial.i] <- selection.i # Actual selection
outcome.v[trial.i] <- outcome.i     # Actual outcome
reward.mtx[trial.i, selection.i] <- outcome.i
gt.mtx[trial.i,] <- p.getthere
}
# Put main results in a single dataframe called sim.result.df
sim.result.df <- data.frame("selection" = selection.v,
"outcome" = outcome.v,
"outcome.cum" = cumsum(outcome.v),
stringsAsFactors = FALSE)
gt.mtx
trial.i <- 2
trial.i <- 1
# STEP 0: Get prior expectations for current trial
exp.prior.i <- exp.prior.mtx[trial.i,]
# Determine probability of reaching goal for each option
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
sd.observed <- sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed[is.na(sd.observed)] <- prior.sd.start
trials.left <- n.trials - trial.i
p.getthere <- p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
p.getthere
trial.i <- 1
exp.prior.i <- exp.prior.mtx[trial.i,]
exp.prior.i
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
points.earned
outcome.v
# Get some game parameters from inputs
n.options <- length(option.mean)
# Create outcome matrix giving the outcome on each trial for each option
outcome.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
for(option.i in 1:n.options) {
outcome.mtx[,option.i] <- rnorm(n = n.trials,
mean = option.mean[option.i],
sd = option.sd[option.i])
}
# Create exp.prior and exp.new matrices
#  These will hold the agent's expectation (either prior or new)
#   of each option on each trial
exp.prior.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
exp.prior.mtx[1,] <- prior.exp.start
exp.new.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# Now create some matrices to store values
selection.v <- rep(NA, n.trials)      # Actual selections
outcome.v <- rep(NA, n.trials)        # Actual outcomes
selprob.mtx <- as.data.frame(matrix(NA,             # Selection probabilities
nrow = n.trials,
ncol = n.options))
names(selprob.mtx) <- paste0("sel.", letters[1:n.options])
gt.mtx <- as.data.frame(matrix(NA,             # p get there
nrow = n.trials,
ncol = n.options))
names(gt.mtx) <- paste0("gt.", letters[1:n.options])
reward.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# RUN SIMULATION!
for(trial.i in 1:n.trials) {
# STEP 0: Get prior expectations for current trial
exp.prior.i <- exp.prior.mtx[trial.i,]
# Determine probability of reaching goal for each option
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
sd.observed <- sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed[is.na(sd.observed)] <- prior.sd.start
trials.left <- n.trials - trial.i
p.getthere <- p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
# STEP 1: SELECT AN OPTION
# Selection probabilities
# EV strategy: Select according to expectations
if(strategy == "ev") {
if(selection.strat == "softmax") {
selprob.i <- softmax.fun(exp.current = exp.prior.i,
theta = theta)
}
if(selection.strat == "egreedy") {
selprob.i <- egreedy.fun(exp.current = exp.prior.i,
epsilon = epsilon)
}
}
# RSF strategy: Select according to probability of reaching the goal
if(strategy == "rsf") {
if(selection.strat == "softmax") {
if(mean(is.finite(p.getthere)) == 1) {
if(sum(p.getthere) == 0) {
selprob.i <- rep(1 / n.options, n.options)} else {
selprob.i <- p.getthere / sum(p.getthere)
}
} else {selprob.i <- rep(1 / n.options, n.options)}
}
if(selection.strat == "egreedy") {
selprob.i <- egreedy.fun(p.getthere, epsilon)
}
}
# Select an option
selection.i <- sample(1:n.options, size = 1, prob = selprob.i)
# Get outcome from selected option
outcome.i <- outcome.mtx[trial.i, selection.i]
# STEP 3: CREATE NEW EXPECTANCIES
# Create a new.inf vector with NAs except for outcome of selected option
new.inf <- rep(NA, n.options)
new.inf[selection.i] <- outcome.i
# Get new expectancies
new.exp.i <- rw.fun(exp.prior = exp.prior.i,
new.inf = new.inf,
alpha = alpha)
# assign new expectatations to exp.new.mtx[trial.i,]
#  and prior.expecation.mtx[trial.i + 1,]
exp.new.mtx[trial.i,] <- new.exp.i
if(trial.i < n.trials) {
exp.prior.mtx[trial.i + 1,] <- new.exp.i
}
# Save some values
selprob.mtx[trial.i,] <- selprob.i  # Selection probabilities
selection.v[trial.i] <- selection.i # Actual selection
outcome.v[trial.i] <- outcome.i     # Actual outcome
reward.mtx[trial.i, selection.i] <- outcome.i
gt.mtx[trial.i,] <- p.getthere
}
trial.i <- 1
exp.prior.i <- exp.prior.mtx[trial.i,]
exp.prior.i
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.earned
outcome.v
# TESTING
n.trials = 25                # Trials in game
option.mean = c(2, 3)   # Mean of each option
option.sd = c(1, 5)       # SD of each option
prior.exp.start = rep(0, 2)  # Prior expectations
prior.sd.start = 1            # Prior standard deviation
goal = 50                    # Goal
epsilon = .1                 # epsilon parameter for egreedy.fun
theta = .5                    # theta parameter for softmax.fun
alpha = .2                    # alpha updating rate for rw.fun
selection.strat = "softmax"   # softmax or egreedy
strategy = "rsf"               # Either ev or rsf
plot = FALSE
ylim = NULL
# Get some game parameters from inputs
n.options <- length(option.mean)
# Create outcome matrix giving the outcome on each trial for each option
outcome.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
for(option.i in 1:n.options) {
outcome.mtx[,option.i] <- rnorm(n = n.trials,
mean = option.mean[option.i],
sd = option.sd[option.i])
}
# Create exp.prior and exp.new matrices
#  These will hold the agent's expectation (either prior or new)
#   of each option on each trial
exp.prior.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
exp.prior.mtx[1,] <- prior.exp.start
exp.new.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# Now create some matrices to store values
selection.v <- rep(NA, n.trials)      # Actual selections
outcome.v <- rep(NA, n.trials)        # Actual outcomes
selprob.mtx <- as.data.frame(matrix(NA,             # Selection probabilities
nrow = n.trials,
ncol = n.options))
names(selprob.mtx) <- paste0("sel.", letters[1:n.options])
gt.mtx <- as.data.frame(matrix(NA,             # p get there
nrow = n.trials,
ncol = n.options))
names(gt.mtx) <- paste0("gt.", letters[1:n.options])
reward.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# RUN SIMULATION!
outcome.v
trial.i
exp.prior.i <- exp.prior.mtx[trial.i,]
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
sum(outcome.v[1:trial.i], na.rm = TRUE)
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed
sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed <- sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed[is.na(sd.observed)] <- prior.sd.start
sd.observed
trials.left <- n.trials - trial.i
p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
trials.left <- n.trials - trial.i
p.getthere <- p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
mean(is.finite(p.getthere)) == 1
eg
]
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
# get list of data
list.games <- list.files("data/", pattern = "_g.csv")
list.surveys <- list.files("data/", pattern = "_s.csv")
list.games <- paste0("data/", list.games)
list.surveys <- paste0("data/", list.surveys)
df.trial <- readRDS("data/dataTrialLevel.rds")
df.game <- readRDS("data/dataGameLevel.rds")
df.participant <- readRDS("data/dataParticipantLevel.rds")
df.pgetthere <- readRDS("data/dataTrialLevelPGetthere.rds")
library(yarrr)
library(dplyr)
library(BayesFactor)
head(df.game)
head(df.trial)
with(df.trial, tapply(high.var.chosen, INDEX = list(overGoal, condition), FUN = mean, na.rm = TRUE))

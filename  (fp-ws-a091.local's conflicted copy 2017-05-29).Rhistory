exp.prior.i <- exp.prior.mtx[trial.i,]
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
sum(outcome.v[1:trial.i], na.rm = TRUE)
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed
sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed <- sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed[is.na(sd.observed)] <- prior.sd.start
sd.observed
trials.left <- n.trials - trial.i
p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
trials.left <- n.trials - trial.i
p.getthere <- p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
mean(is.finite(p.getthere)) == 1
eg
]
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .5)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
egreedy.fun(exp.current = c(0, 0), epsilon = .2)
# get list of data
list.games <- list.files("data/", pattern = "_g.csv")
list.surveys <- list.files("data/", pattern = "_s.csv")
list.games <- paste0("data/", list.games)
list.surveys <- paste0("data/", list.surveys)
df.trial <- readRDS("data/dataTrialLevel.rds")
df.game <- readRDS("data/dataGameLevel.rds")
df.participant <- readRDS("data/dataParticipantLevel.rds")
df.pgetthere <- readRDS("data/dataTrialLevelPGetthere.rds")
library(yarrr)
library(dplyr)
library(BayesFactor)
head(df.game)
head(df.trial)
with(df.trial, tapply(high.var.chosen, INDEX = list(overGoal, condition), FUN = mean, na.rm = TRUE))
library(shiny); runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
install.packages("shinyjs")
library(shiny); runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
df <- readRDS("data/dataTrialLevelPGetthere.rds")
head(df)
table(df$high.var.chosen[df$condition == 3], df$choose.highvar.subj[df$condition == 3])
#
# Learning functions
#
# UPDATING
# rw.fun: Rescorla-Wagner
#
# CHOICE
# softmax.fun(exp.current, theta): Softmax
# egreedy.fun(exp.current, epsilon): epsilon-greedy
#
# OTHER
# p.getthere.fun(points.needed, trials.left, mu, sigma): What is the probability of reaching a goal?
#
# SIMULATION
#
# rl.sim.fun(): Simulate the behavior of an agent given specified parameters
#
# Rescorla-Wagner prediction error updating function
rw.fun <- function(exp.prior = c(5, 3, 7),      # A vector of prior expectations
new.inf = c(NA, 2, NA),        # A vector of new information (NAs except for selected option)
alpha = .3) {   # Updating rate
# Save new expectations as prior
exp.new <- exp.prior
# Determine which option was selected
selection <- which(is.finite(new.inf))
# Update expectation of selected option
exp.new[selection] <- exp.prior[selection] + alpha * (new.inf[selection] - exp.prior[selection])
return(exp.new)
}
# Softmax selection function
softmax.fun <- function(exp.current = c(5, 3, 6),   # A vector of expectations
theta = .5,                  # Choice sensitivity
trial = NA
) {
if(is.na(trial)) {
output <- exp(exp.current * theta) / sum(exp(exp.current * theta))
}
return(output)
}
# egreedy
egreedy.fun <- function(exp.current = c(5, 3, 6),   # A vector of expectations
epsilon = .5                # probability of random choice
) {
n.options <- length(exp.current)
output <- rep(0,n.options)
best <- which(exp.current == max(exp.current))
if(length(best) > 1) {best <- sample(best, 1)}
# exploit or explore?
# If random number is less than epsilon, then explore
if(runif(1, 0, 1) < epsilon) {
if(n.options == 2) {
selection <- setdiff(1:n.options, best)
} else {
selection <- sample(setdiff(1:n.options, best), size = 1)
}
} else {  # exploit
selection <- best
}
output[selection] <- 1
return(output)
}
# What is the probability I will reach the goal?
p.getthere.fun <- function(points.needed,  # How many points do I need?
trials.left,     # Trials remaining
mu,             # Mean of distribution(s)
sigma) {        # SD of distribution(s)
n.options <- length(mu)
output <- sapply(1:n.options, FUN = function(x) {
1 - pnorm(q = points.needed,                     # points desired
mean = mu[x] * trials.left,            # Mean
sd = sqrt(trials.left * sigma[x] ^ 2)) # Sd
})
return(output)
}
# Create main simulation function
rl.sim.fun <- function(n.trials = 100,                # Trials in game
option.mean = c(.5, 1, 1.5),   # Mean of each option
option.sd = c(.1, 2, 4),       # SD of each option
prior.exp.start = rep(0, 3),   # Prior expectations
prior.sd.start = 1,            # Prior standard deviation
goal = 100,                    # Goal
epsilon = .1,                  # epsilon parameter for egreedy.fun
theta = .5,                    # theta parameter for softmax.fun
alpha = .2,                    # alpha updating rate for rw.fun
selection.strat = "egreedy",   # softmax or egreedy
strategy = "ev",               # Either ev or rsf
int.values = FALSE,
plot = FALSE,
ylim = NULL) {
# TESTING
# n.trials = 25                # Trials in game
# option.mean = c(2, 3)   # Mean of each option
# option.sd = c(1, 5)       # SD of each option
# prior.exp.start = rep(0, 2)  # Prior expectations
# prior.sd.start = 1            # Prior standard deviation
# goal = 50                    # Goal
# epsilon = .1                 # epsilon parameter for egreedy.fun
# theta = .5                    # theta parameter for softmax.fun
# alpha = .2                    # alpha updating rate for rw.fun
# selection.strat = "softmax"   # softmax or egreedy
# strategy = "rsf"               # Either ev or rsf
# plot = FALSE
# ylim = NULL
# Get some game parameters from inputs
n.options <- length(option.mean)
# Create outcome matrix giving the outcome on each trial for each option
outcome.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
if (int.values == FALSE){
for(option.i in 1:n.options) {
outcome.mtx[,option.i] <- rnorm(n = n.trials,
mean = option.mean[option.i],
sd = option.sd[option.i])
}
} else {
for(option.i in 1:n.options) {
outcome.mtx[,option.i] <- round(rnorm(n = n.trials,
mean = option.mean[option.i],
sd = option.sd[option.i]))
}
}
# Create exp.prior and exp.new matrices
#  These will hold the agent's expectation (either prior or new)
#   of each option on each trial
exp.prior.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
exp.prior.mtx[1,] <- prior.exp.start
exp.new.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# Now create some matrices to store values
selection.v <- rep(NA, n.trials)      # Actual selections
outcome.v <- rep(NA, n.trials)        # Actual outcomes
selprob.mtx <- as.data.frame(matrix(NA,             # Selection probabilities
nrow = n.trials,
ncol = n.options))
names(selprob.mtx) <- paste0("sel.", letters[1:n.options])
gt.mtx <- as.data.frame(matrix(NA,             # p get there
nrow = n.trials,
ncol = n.options))
names(gt.mtx) <- paste0("gt.", letters[1:n.options])
reward.mtx <- matrix(NA, nrow = n.trials, ncol = n.options)
# RUN SIMULATION!
for(trial.i in 1:n.trials) {
# STEP 0: Get prior expectations for current trial
exp.prior.i <- exp.prior.mtx[trial.i,]
# Determine probability of reaching goal for each option
points.earned <- sum(outcome.v[1:trial.i], na.rm = TRUE)
points.needed <- goal - sum(outcome.v[1:trial.i], na.rm = TRUE)
sd.observed <- sapply(1:n.options, FUN = function(x) {sd(reward.mtx[,x], na.rm = TRUE)})
sd.observed[is.na(sd.observed)] <- prior.sd.start
trials.left <- n.trials - trial.i
p.getthere <- p.getthere.fun(points.needed = points.needed,
trials.left = trials.left,
mu = exp.prior.i,
sigma = sd.observed)
# STEP 1: SELECT AN OPTION
# Selection probabilities
# EV strategy: Select according to expectations
if(strategy == "ev") {
if(selection.strat == "softmax") {
selprob.i <- softmax.fun(exp.current = exp.prior.i,
theta = theta)
}
if(selection.strat == "egreedy") {
selprob.i <- egreedy.fun(exp.current = exp.prior.i,
epsilon = epsilon)
}
}
# RSF strategy: Select according to probability of reaching the goal
if(strategy == "rsf") {
if(selection.strat == "softmax") {
if(mean(is.finite(p.getthere)) == 1) {
if(sum(p.getthere) == 0) {
selprob.i <- rep(1 / n.options, n.options)} else {
selprob.i <- p.getthere / sum(p.getthere)
}
} else {selprob.i <- rep(1 / n.options, n.options)}
}
if(selection.strat == "egreedy") {
selprob.i <- egreedy.fun(p.getthere, epsilon)
}
}
# Select an option
selection.i <- sample(1:n.options, size = 1, prob = selprob.i)
# Get outcome from selected option
outcome.i <- outcome.mtx[trial.i, selection.i]
# STEP 3: CREATE NEW EXPECTANCIES
# Create a new.inf vector with NAs except for outcome of selected option
new.inf <- rep(NA, n.options)
new.inf[selection.i] <- outcome.i
# Get new expectancies
new.exp.i <- rw.fun(exp.prior = exp.prior.i,
new.inf = new.inf,
alpha = alpha)
# assign new expectatations to exp.new.mtx[trial.i,]
#  and prior.expecation.mtx[trial.i + 1,]
exp.new.mtx[trial.i,] <- new.exp.i
if(trial.i < n.trials) {
exp.prior.mtx[trial.i + 1,] <- new.exp.i
}
# Save some values
selprob.mtx[trial.i,] <- selprob.i  # Selection probabilities
selection.v[trial.i] <- selection.i # Actual selection
outcome.v[trial.i] <- outcome.i     # Actual outcome
reward.mtx[trial.i, selection.i] <- outcome.i
gt.mtx[trial.i,] <- p.getthere
}
# Put main results in a single dataframe called sim.result.df
sim.result.df <- data.frame("selection" = selection.v,
"outcome" = outcome.v,
"outcome.cum" = cumsum(outcome.v),
stringsAsFactors = FALSE)
sim.result.df <- cbind(sim.result.df, selprob.mtx, gt.mtx)
sim.result.df
# Should simulation be plotted?
if(plot == TRUE) {
if(is.null(ylim)) {ylim <- c(min(option.mean) * n.trials,  # Set limits to worst and best exepected performance
max(option.mean) * n.trials)}
plot(1,
xlim = c(1, n.trials),
ylim = ylim,
type = "n",
ylab = "Cumulative Rewards",
xlab = "Trial",
main = paste0("alpha = ", alpha, ", theta = ", theta))
rect(-1e3, -1e3, 1e3, 1e3, col = gray(.96))
abline(h = seq(-1e3, 1e3, by = 10),
v = seq(-1e3, 1e3, by = 10),
lwd = c(2, 1), col = "white")
points(x = 1:n.trials,
y = sim.result.df$outcome.cum,
type = "b")
text(1:n.trials,
y = sim.result.df$outcome.cum,
labels = letters[selection.v], pos = 3)
if(is.finite(goal)) {abline(h = goal)}
}
# Now return the main simulation dataframe
return(sim.result.df)
}
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1)
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
egreedy.fun(epsilon = 1, exp.current = c(10000, 0, -100))
exp.prior.i <- c(1000, 0, -200)
theta <- 0
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
theta <- 1
softmax.fun(exp.current = exp.prior.i,
theta = theta)
softmax.fun(exp.current = exp.prior.i,
theta = theta)
theta <- .1
softmax.fun(exp.current = exp.prior.i,
theta = theta)
library(shiny); runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
install.packages("shinyjs")
library(shinyjs)
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
install.packages("rdrop2")
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
runApp('Apps/javaBanditNoGoal/javaBanditNoGoal_app.R')
runApp('Apps/javaBanditNoGoal/javaBanditNoGoal_app.R')
runApp('Apps/javaBanditNoGoal/javaBanditNoGoal_app.R')
library(shiny); runApp('Apps/GoalQuestionnaire/GoalQuestionnaire_app.R')
library(shiny); runApp('Apps/GoalQuestionnaire/GoalQuestionnaire_app.R')
runApp('Apps/NoGoalQuestionnaire/NoGoalQuestionnaire_app.R')
runApp('Apps/NoGoalQuestionnaire/NoGoalQuestionnaire_app.R')
runApp('Apps/javaBanditGoal/javaBanditGoal_app.R')
cranlogs::cran_downloads("FFTrees", "last-week")
cranlogs::cran_downloads("yarrr", "last-week")
library(yarrr)
pirateplot(weight ~ Diet, data = ChickWeight)
pirateplot(weight ~ Time, data = ChickWeight)
pirateplot(weight ~ Time, data = ChickWeight, bar.b.o = .3)
pirateplot(weight ~ Time, data = ChickWeight, bar.b.o = .8, bar.f.o = .3)
pirateplot(weight ~ Time, data = ChickWeight, bar.b.o = .8, bar.f.o = .3, bar.f.col = "black")
pirateplot(weight ~ Time, data = ChickWeight, bar.b.o = .8, bar.f.o = .3, bar.f.col = "black", theme = 2)
pirateplot(weight ~ Time, data = ChickWeight, bar.b.o = .8, bar.f.o = .3, bar.f.col = "black", theme = 3)
pirateplot(weight ~ Time, data = ChickWeight, bar.b.o = .8, bar.f.o = .3, bar.f.col = "black", theme = 4)
pirateplot(weight ~ Diet, data = ChickWeight, bar.b.o = .8, bar.f.o = .3, bar.f.col = "black", theme = 4)
library(FFTrees)
x <- FFTrees(diagnosis ~., data = heartdisease, train.p = .5)
x
plot(x)
plot(x, "test")
# --------------------------
# Section 0: Load Libraries
# --------------------------
if (!require(yarrr)) install.packages("yarrr"); library(yarrr)
if (!require(BayesFactor)) install.packages("BayesFactor"); library(BayesFactor)
if (!require(lsr)) install.packages("lsr"); library(lsr)
# --------------------------
# Section 0: Load Libraries
# --------------------------
if (!require(yarrr)) install.packages("yarrr"); library(yarrr)
if (!require(BayesFactor)) install.packages("BayesFactor"); library(BayesFactor)
if (!require(lsr)) install.packages("lsr"); library(lsr)
if (!require(coin)) install.packages("coin"); library(coin)
if (!require(lme4)) install.packages("lme4"); library(lme4)
if (!require(dplyr)) install.packages("dplyr"); library(dplyr)
# Set working directory
setwd(rprojroot::is_rstudio_project$find_file())
# load dataframes
df.trial <- readRDS("data/SimulationData/useData/S1_dataTrialLevel.rds")
df.game <- readRDS("data/SimulationData/useData/S1_dataGameLevel.rds")
df.participant <- readRDS("data/SimulationData/useData/S1_dataParticipantLevel.rds")
# ----------------------
# Section A: Game Data
# ----------------------
### Section A1: Descriptive Analyses
summary(df.trial)
# number of participants per condition
table(df.participant$goal.condition, df.participant$variance.condition)
View(df.participant)
m.rug <- glmer(high.var.chosen ~ variance.condition + goal.condition + (1|game) + (1|id),
data = subset(df.trial, overGoal == 0 & game > 1), family = binomial)
summary(m.rug)
exp(m.rug@beta[2])
exp(m.rug@beta[3])
exp(m.rug@beta[4])
pirateplot(risky.ug ~ goal.condition + variance.condition, data = df.participant,
ylab = "prop high var chosen", xlab = "Conditions", main = "Risky Under Goal")
m.rug <- glmer(high.var.chosen ~ variance.condition * goal.condition + (1|game) + (1|id),
data = subset(df.trial, overGoal == 0 & game > 1), family = binomial)
summary(m.rug)
m.rag <- glmer(high.var.chosen ~ variance.condition + goal.condition + (1|game) + (1|id),
data = subset(df.trial, overGoal == 1 & game > 1), family = binomial)
pirateplot(risky.ag ~ goal.condition + variance.condition, data = df.participant,
ylab = "prop high var chosen", xlab = "Conditions", main = "Risky Above Goal")
summary(m.rag)
df.n <- aggregate(high.var.chosen ~ choose.highvar.subj + id + goal.condition + variance.condition, FUN = mean,
data = subset(df.tria, game > 1))
df.n <- aggregate(high.var.chosen ~ choose.highvar.subj + id + goal.condition + variance.condition, FUN = mean,
data = subset(df.trial, game > 1))
df.n
pirateplot(high.var.chosen ~ choose.highvar.subj + variance.condition + goal.condition, data = df.n,
ylab = "prop high var chosen", xlab = "choose high var subj (rsf)", main = "")
m.chv <- glmer(high.var.chosen ~ choose.highvar.subj + (1|game) + (1|id),
data = subset(df.trial, goal.condition == "NoGoal" & game > 1), family = binomial)
summary(m.chv)
m.chv <- glmer(high.var.chosen ~ choose.highvar.subj + (1|game) + (1|id),
data = subset(df.trial, goal.condition == "Goal" & game > 1), family = binomial)
summary(m.chv)
with(subset(df.trial, game > 1), mean(pred.EV != pred.RSF, na.rm = TRUE))
m.pa <- glmer(pred.RSF.acc ~ goal.condition + (1|game) + (1|id),
data = subset(df.trial, game > 1 & pred.EV != pred.RSF), family = binomial)
summary(m.pa)
df.p <- df.trial %>%
filter(game > 1 & pred.EV != pred.RSF) %>%
group_by(id, variance.condition, goal.condition) %>%
summarise(
pred.EV.acc.rate = mean(pred.EV.acc, na.rm = TRUE),
pred.RSF.acc.rate = mean(pred.RSF.acc, na.rm = TRUE)
)
if (Sys.info()[1] == "Windows"){
windows(height = 22, width = 33)
} else {
quartz(height = 22, width = 33)
}
par(mar=c(5,6.7,3,1.5))
pirateplot(pred.RSF.acc.rate ~ goal.condition + variance.condition, data = df.p,
ylab = "RSF corr pred rate", xlab = "Condition", main = "Only Trials where RSF and EV differ")
# --------
pirateplot(pred.RSF.acc.rate ~ goal.condition + variance.condition, data = df.p,
ylab = "RSF corr pred rate", xlab = "Condition", main = "Only Trials where RSF and EV differ")
w.hvo <- wilcox_test(high.var.chosen.rate ~ as.factor(goal.condition), data = df.participant); w.hvo
?wilcox_test
install.packages("coin")
install.packages("coin")
install.packages("coin")
pirateplot(high.var.chosen.rate ~ goal.condition, data = df.participant,
ylab = "prop high var chosen", xlab = "Conditions", main = "Risky Rate Overall")
pirateplot(high.var.chosen.rate ~ goal.condition + variance.condition, data = df.participant,
ylab = "prop high var chosen", xlab = "Conditions", main = "Risky Rate Overall")
pirateplot(goalReachedRate ~ goal.condition, data = df.participant,
ylab = "prop high var chosen", xlab = "Conditions", main = "Goal Reached Rate")
pirateplot(goalReachedRate ~ goal.condition, data = df.participant,
ylab = "Reach goal", xlab = "Conditions", main = "Goal Reached Rate")
pirateplot(goalReachedRate ~ goal.condition + variance.conditino, data = df.participant,
ylab = "Reach goal", xlab = "Conditions", main = "Goal Reached Rate")
pirateplot(goalReachedRate ~ goal.condition + variance.condition, data = df.participant,
ylab = "Reach goal", xlab = "Conditions", main = "Goal Reached Rate")
pirateplot(points.cum ~ variance.condition + goal.condition, data = df.participant,
ylab = "Total Number of Points", xlab = "Conditions", main = "Total Points Reached")
pirateplot(points.cum ~ goal.condition + variance.condition, data = df.participant,
ylab = "Total Number of Points", xlab = "Conditions", main = "Total Points Reached")
library(FFTrees)
FFTrees_guide()
devtools::install_github("ndphillips/FFTrees", build_vignettes = TRUE)
cranlogs::cran_downloads("FFTrees", "last-week")
cranlogs::cran_downloads("FFTrees", "last-week")
